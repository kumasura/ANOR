{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObMphPYbubk/HlC5IVIYjH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumasura/ANOR/blob/main/FlightRerouting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZF_SacieJ_DA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FlightReroutingEnv(gym.Env):\n",
        "\n",
        "    \"\"\"Environment using schedule data with simple simulated disruptions.\"\"\"\n",
        "    def __init__(self, schedule_path, disruption_prob=0.3):\n",
        "\n",
        "        super().__init__()\n",
        "        self.schedule = pd.read_excel(schedule_path)\n",
        "        self.num_flights = len(self.schedule)\n",
        "        self.disruption_prob = disruption_prob\n",
        "\n",
        "\n",
        "        # Observation consists of: flight index, fuel level, weather, traffic,\n",
        "        # alternate airports, other aircraft proximity\n",
        "        self.obs_bins = np.array([\n",
        "            self.num_flights + 1,  # flight index including terminal state\n",
        "            5,  # fuel level bins\n",
        "            5,  # weather bins\n",
        "            5,  # traffic bins\n",
        "            4,  # number of alternate airports\n",
        "            5,  # other aircraft proximity bins\n",
        "        ])\n",
        "        self.observation_space = spaces.MultiDiscrete(self.obs_bins)\n",
        "\n",
        "        # Actions follow the README: change path, swap aircraft, cancel, adjust\n",
        "        # altitude, divert, wait for conditions to improve\n",
        "        self.action_space = spaces.Discrete(6)\n",
        "\n",
        "    def _random_state(self):\n",
        "        return [\n",
        "            np.random.randint(self.obs_bins[1]),\n",
        "            np.random.randint(self.obs_bins[2]),\n",
        "            np.random.randint(self.obs_bins[3]),\n",
        "            np.random.randint(self.obs_bins[4]),\n",
        "            np.random.randint(self.obs_bins[5]),\n",
        "        ]\n",
        "\n",
        "\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_idx = 0\n",
        "\n",
        "        self.state = self._random_state()\n",
        "        return tuple([self.current_idx] + self.state), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.current_idx >= self.num_flights:\n",
        "            raise RuntimeError(\"Episode is done\")\n",
        "\n",
        "        disruption = np.random.rand() < self.disruption_prob\n",
        "        fuel, weather, traffic, airports, other = self.state\n",
        "\n",
        "        # Simple cost model\n",
        "        fuel_cost = 10 + 5 * weather + 5 * traffic\n",
        "        delay_penalty = 20 if action == 5 else 0\n",
        "        swap_penalty = 50 if action == 1 else 0\n",
        "        cancel_penalty = 200 if action == 2 else 0\n",
        "        reroute_penalty = 30 if action in (0, 3, 4) else 0\n",
        "        if not disruption and action == 5:\n",
        "            delay_penalty += 20  # unnecessary waiting\n",
        "\n",
        "        reward = -(fuel_cost + delay_penalty + swap_penalty + cancel_penalty + reroute_penalty)\n",
        "\n",
        "        # Advance to next flight and generate new state\n",
        "        self.current_idx += 1\n",
        "        terminated = self.current_idx >= self.num_flights\n",
        "        self.state = self._random_state() if not terminated else [0] * 5\n",
        "        obs = tuple([self.num_flights] + self.state) if terminated else tuple([self.current_idx] + self.state)\n",
        "        return obs, reward, terminated, False, {}"
      ],
      "metadata": {
        "id": "c6aVg2RrMihZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, nvec, action_size, alpha=0.1, gamma=0.95, epsilon=0.1):\n",
        "        state_space = int(np.prod(nvec))\n",
        "        self.Q = np.zeros((state_space, action_size))\n",
        "        self.nvec = nvec\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.action_size = action_size\n",
        "\n",
        "\n",
        "    def _state_index(self, state):\n",
        "        return np.ravel_multi_index(state, self.nvec)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        idx = self._state_index(state)\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "        return int(np.argmax(self.Q[idx]))\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        idx = self._state_index(state)\n",
        "        next_idx = self._state_index(next_state)\n",
        "        best_next = np.max(self.Q[next_idx])\n",
        "        td_target = reward + self.gamma * best_next\n",
        "        td_error = td_target - self.Q[idx, action]\n",
        "        self.Q[idx, action] += self.alpha * td_error\n"
      ],
      "metadata": {
        "id": "FaO_JzKBMz7m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(env, agent):\n",
        "    \"\"\"Run one episode with the learned policy and print step details.\"\"\"\n",
        "    action_names = [\n",
        "        \"Change path\",\n",
        "        \"Swap aircraft\",\n",
        "        \"Cancel flight\",\n",
        "        \"Adjust altitude\",\n",
        "        \"Divert\",\n",
        "        \"Wait\",\n",
        "    ]\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action = int(np.argmax(agent.Q[agent._state_index(state)]))\n",
        "        next_state, reward, terminated, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        print(\n",
        "            f\"Flight {state[0]} -> action: {action_names[action]} | \"\n",
        "            f\"state: {state[1:]} | reward: {reward:.1f}\"\n",
        "        )\n",
        "        state = next_state\n",
        "        done = terminated\n",
        "    print(\"Total reward:\", total_reward)\n"
      ],
      "metadata": {
        "id": "ifg1KjJoO1PQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env, episodes=200):\n",
        "    agent = QLearningAgent(env.obs_bins, env.action_space.n)\n",
        "    for _ in range(episodes):\n",
        "\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, _, _ = env.step(action)\n",
        "            agent.update(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "            done = terminated\n",
        "    return agent"
      ],
      "metadata": {
        "id": "Cj6Tiq6oM3OG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    env = FlightReroutingEnv('flight_schedule_new.xlsx')\n",
        "    agent = train(env)\n",
        "    print(\"Q-table shape:\", agent.Q.shape)\n",
        "    print(agent.Q[:5])\n",
        "    print(\"\\nPolicy rollout:\")\n",
        "    evaluate(env, agent)\n"
      ],
      "metadata": {
        "id": "ZZqzflnFM5r2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtTVyY5bM72m",
        "outputId": "bca314fe-8b08-4032-d7f3-6519544b3df7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-table shape: (395000, 6)\n",
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "Policy rollout:\n",
            "Flight 0 -> action: Change path | state: (2, 3, 1, 1, 3) | reward: -60.0\n",
            "Flight 1 -> action: Change path | state: (0, 4, 1, 0, 0) | reward: -65.0\n",
            "Flight 2 -> action: Change path | state: (1, 2, 3, 0, 3) | reward: -65.0\n",
            "Flight 3 -> action: Change path | state: (3, 0, 0, 0, 3) | reward: -40.0\n",
            "Flight 4 -> action: Change path | state: (3, 4, 4, 0, 4) | reward: -80.0\n",
            "Flight 5 -> action: Change path | state: (2, 2, 3, 2, 2) | reward: -65.0\n",
            "Flight 6 -> action: Change path | state: (0, 2, 3, 1, 4) | reward: -65.0\n",
            "Flight 7 -> action: Change path | state: (0, 0, 3, 0, 4) | reward: -55.0\n",
            "Flight 8 -> action: Change path | state: (2, 4, 4, 0, 0) | reward: -80.0\n",
            "Flight 9 -> action: Change path | state: (2, 3, 2, 1, 4) | reward: -65.0\n",
            "Flight 10 -> action: Change path | state: (3, 1, 0, 0, 4) | reward: -45.0\n",
            "Flight 11 -> action: Change path | state: (2, 4, 0, 1, 3) | reward: -60.0\n",
            "Flight 12 -> action: Change path | state: (4, 2, 3, 0, 0) | reward: -65.0\n",
            "Flight 13 -> action: Change path | state: (4, 3, 4, 0, 0) | reward: -75.0\n",
            "Flight 14 -> action: Change path | state: (3, 4, 3, 2, 0) | reward: -75.0\n",
            "Flight 15 -> action: Change path | state: (0, 2, 4, 3, 2) | reward: -70.0\n",
            "Flight 16 -> action: Change path | state: (1, 0, 1, 3, 0) | reward: -45.0\n",
            "Flight 17 -> action: Change path | state: (1, 1, 0, 1, 4) | reward: -45.0\n",
            "Flight 18 -> action: Change path | state: (2, 3, 3, 2, 4) | reward: -70.0\n",
            "Flight 19 -> action: Change path | state: (4, 4, 3, 2, 1) | reward: -75.0\n",
            "Flight 20 -> action: Change path | state: (0, 4, 0, 2, 0) | reward: -60.0\n",
            "Flight 21 -> action: Change path | state: (1, 0, 0, 1, 4) | reward: -40.0\n",
            "Flight 22 -> action: Change path | state: (0, 1, 3, 3, 4) | reward: -60.0\n",
            "Flight 23 -> action: Change path | state: (0, 1, 0, 1, 3) | reward: -45.0\n",
            "Flight 24 -> action: Change path | state: (2, 0, 2, 3, 3) | reward: -50.0\n",
            "Flight 25 -> action: Change path | state: (1, 1, 1, 3, 2) | reward: -50.0\n",
            "Flight 26 -> action: Change path | state: (4, 1, 0, 3, 0) | reward: -45.0\n",
            "Flight 27 -> action: Change path | state: (3, 3, 0, 2, 3) | reward: -55.0\n",
            "Flight 28 -> action: Change path | state: (0, 1, 4, 1, 1) | reward: -65.0\n",
            "Flight 29 -> action: Change path | state: (4, 3, 4, 3, 0) | reward: -75.0\n",
            "Flight 30 -> action: Change path | state: (3, 4, 2, 2, 2) | reward: -70.0\n",
            "Flight 31 -> action: Change path | state: (2, 3, 3, 0, 2) | reward: -70.0\n",
            "Flight 32 -> action: Change path | state: (2, 1, 3, 2, 0) | reward: -60.0\n",
            "Flight 33 -> action: Change path | state: (4, 1, 0, 1, 1) | reward: -45.0\n",
            "Flight 34 -> action: Change path | state: (3, 0, 4, 1, 0) | reward: -60.0\n",
            "Flight 35 -> action: Change path | state: (4, 3, 1, 1, 1) | reward: -60.0\n",
            "Flight 36 -> action: Change path | state: (1, 0, 3, 2, 0) | reward: -55.0\n",
            "Flight 37 -> action: Change path | state: (1, 4, 4, 0, 3) | reward: -80.0\n",
            "Flight 38 -> action: Change path | state: (1, 0, 4, 1, 3) | reward: -60.0\n",
            "Flight 39 -> action: Change path | state: (1, 3, 1, 2, 2) | reward: -60.0\n",
            "Flight 40 -> action: Change path | state: (3, 0, 1, 3, 2) | reward: -45.0\n",
            "Flight 41 -> action: Change path | state: (3, 2, 1, 2, 3) | reward: -55.0\n",
            "Flight 42 -> action: Change path | state: (3, 1, 2, 2, 3) | reward: -55.0\n",
            "Flight 43 -> action: Change path | state: (0, 3, 1, 1, 0) | reward: -60.0\n",
            "Flight 44 -> action: Change path | state: (2, 2, 2, 3, 4) | reward: -60.0\n",
            "Flight 45 -> action: Change path | state: (0, 1, 0, 0, 3) | reward: -45.0\n",
            "Flight 46 -> action: Change path | state: (3, 2, 1, 0, 2) | reward: -55.0\n",
            "Flight 47 -> action: Change path | state: (3, 3, 2, 3, 0) | reward: -65.0\n",
            "Flight 48 -> action: Change path | state: (1, 0, 0, 0, 4) | reward: -40.0\n",
            "Flight 49 -> action: Change path | state: (4, 4, 0, 0, 1) | reward: -60.0\n",
            "Flight 50 -> action: Change path | state: (1, 2, 0, 3, 4) | reward: -50.0\n",
            "Flight 51 -> action: Change path | state: (2, 2, 3, 1, 0) | reward: -65.0\n",
            "Flight 52 -> action: Change path | state: (1, 3, 1, 0, 0) | reward: -60.0\n",
            "Flight 53 -> action: Change path | state: (3, 3, 0, 0, 1) | reward: -55.0\n",
            "Flight 54 -> action: Change path | state: (0, 2, 1, 0, 2) | reward: -55.0\n",
            "Flight 55 -> action: Change path | state: (3, 3, 1, 3, 3) | reward: -60.0\n",
            "Flight 56 -> action: Swap aircraft | state: (2, 4, 0, 3, 1) | reward: -80.0\n",
            "Flight 57 -> action: Change path | state: (4, 4, 0, 1, 1) | reward: -60.0\n",
            "Flight 58 -> action: Change path | state: (0, 4, 4, 0, 2) | reward: -80.0\n",
            "Flight 59 -> action: Change path | state: (4, 1, 2, 2, 3) | reward: -55.0\n",
            "Flight 60 -> action: Change path | state: (3, 0, 3, 1, 4) | reward: -55.0\n",
            "Flight 61 -> action: Change path | state: (1, 1, 4, 0, 3) | reward: -65.0\n",
            "Flight 62 -> action: Change path | state: (0, 3, 0, 3, 3) | reward: -55.0\n",
            "Flight 63 -> action: Swap aircraft | state: (0, 4, 1, 1, 3) | reward: -85.0\n",
            "Flight 64 -> action: Change path | state: (4, 3, 4, 2, 0) | reward: -75.0\n",
            "Flight 65 -> action: Change path | state: (1, 3, 1, 2, 0) | reward: -60.0\n",
            "Flight 66 -> action: Change path | state: (2, 0, 2, 0, 1) | reward: -50.0\n",
            "Flight 67 -> action: Change path | state: (0, 1, 4, 2, 2) | reward: -65.0\n",
            "Flight 68 -> action: Change path | state: (4, 4, 4, 2, 2) | reward: -80.0\n",
            "Flight 69 -> action: Change path | state: (3, 2, 1, 3, 0) | reward: -55.0\n",
            "Flight 70 -> action: Change path | state: (1, 1, 1, 3, 2) | reward: -50.0\n",
            "Flight 71 -> action: Change path | state: (2, 1, 1, 0, 2) | reward: -50.0\n",
            "Flight 72 -> action: Change path | state: (3, 1, 1, 3, 4) | reward: -50.0\n",
            "Flight 73 -> action: Change path | state: (0, 1, 0, 3, 2) | reward: -45.0\n",
            "Flight 74 -> action: Change path | state: (1, 2, 3, 2, 3) | reward: -65.0\n",
            "Flight 75 -> action: Change path | state: (3, 2, 2, 0, 3) | reward: -60.0\n",
            "Flight 76 -> action: Change path | state: (3, 0, 0, 3, 4) | reward: -40.0\n",
            "Flight 77 -> action: Change path | state: (3, 1, 1, 2, 4) | reward: -50.0\n",
            "Flight 78 -> action: Change path | state: (1, 1, 2, 1, 0) | reward: -55.0\n",
            "Flight 79 -> action: Change path | state: (0, 3, 0, 0, 0) | reward: -55.0\n",
            "Flight 80 -> action: Change path | state: (4, 1, 0, 2, 3) | reward: -45.0\n",
            "Flight 81 -> action: Change path | state: (0, 3, 2, 1, 0) | reward: -65.0\n",
            "Flight 82 -> action: Change path | state: (2, 4, 4, 1, 1) | reward: -80.0\n",
            "Flight 83 -> action: Change path | state: (2, 4, 3, 3, 1) | reward: -75.0\n",
            "Flight 84 -> action: Change path | state: (2, 1, 2, 3, 4) | reward: -55.0\n",
            "Flight 85 -> action: Change path | state: (4, 2, 1, 2, 4) | reward: -55.0\n",
            "Flight 86 -> action: Change path | state: (4, 0, 4, 3, 2) | reward: -60.0\n",
            "Flight 87 -> action: Change path | state: (0, 0, 2, 2, 0) | reward: -50.0\n",
            "Flight 88 -> action: Change path | state: (0, 1, 3, 1, 2) | reward: -60.0\n",
            "Flight 89 -> action: Change path | state: (3, 1, 4, 2, 2) | reward: -65.0\n",
            "Flight 90 -> action: Change path | state: (2, 4, 1, 2, 0) | reward: -65.0\n",
            "Flight 91 -> action: Change path | state: (0, 3, 4, 3, 0) | reward: -75.0\n",
            "Flight 92 -> action: Change path | state: (0, 3, 3, 2, 3) | reward: -70.0\n",
            "Flight 93 -> action: Change path | state: (3, 3, 1, 3, 2) | reward: -60.0\n",
            "Flight 94 -> action: Change path | state: (2, 3, 2, 2, 3) | reward: -65.0\n",
            "Flight 95 -> action: Change path | state: (4, 0, 0, 1, 2) | reward: -40.0\n",
            "Flight 96 -> action: Change path | state: (0, 1, 0, 1, 1) | reward: -45.0\n",
            "Flight 97 -> action: Change path | state: (2, 2, 2, 3, 4) | reward: -60.0\n",
            "Flight 98 -> action: Change path | state: (2, 0, 1, 0, 0) | reward: -45.0\n",
            "Flight 99 -> action: Change path | state: (3, 0, 0, 0, 2) | reward: -40.0\n",
            "Flight 100 -> action: Change path | state: (2, 3, 3, 0, 4) | reward: -70.0\n",
            "Flight 101 -> action: Change path | state: (1, 4, 4, 2, 3) | reward: -80.0\n",
            "Flight 102 -> action: Change path | state: (1, 2, 0, 3, 0) | reward: -50.0\n",
            "Flight 103 -> action: Change path | state: (1, 1, 1, 3, 4) | reward: -50.0\n",
            "Flight 104 -> action: Change path | state: (0, 0, 3, 3, 0) | reward: -55.0\n",
            "Flight 105 -> action: Change path | state: (2, 0, 0, 1, 4) | reward: -40.0\n",
            "Flight 106 -> action: Change path | state: (1, 2, 3, 3, 2) | reward: -65.0\n",
            "Flight 107 -> action: Change path | state: (4, 0, 2, 3, 0) | reward: -50.0\n",
            "Flight 108 -> action: Change path | state: (0, 4, 3, 3, 1) | reward: -75.0\n",
            "Flight 109 -> action: Change path | state: (4, 3, 3, 1, 4) | reward: -70.0\n",
            "Flight 110 -> action: Change path | state: (0, 4, 4, 2, 0) | reward: -80.0\n",
            "Flight 111 -> action: Change path | state: (1, 4, 0, 3, 1) | reward: -60.0\n",
            "Flight 112 -> action: Change path | state: (0, 0, 2, 0, 2) | reward: -50.0\n",
            "Flight 113 -> action: Change path | state: (4, 4, 4, 3, 2) | reward: -80.0\n",
            "Flight 114 -> action: Change path | state: (1, 3, 3, 0, 2) | reward: -70.0\n",
            "Flight 115 -> action: Change path | state: (0, 2, 2, 2, 2) | reward: -60.0\n",
            "Flight 116 -> action: Change path | state: (0, 2, 4, 2, 3) | reward: -70.0\n",
            "Flight 117 -> action: Change path | state: (2, 4, 0, 1, 1) | reward: -60.0\n",
            "Flight 118 -> action: Change path | state: (3, 0, 2, 1, 2) | reward: -50.0\n",
            "Flight 119 -> action: Change path | state: (0, 3, 0, 3, 4) | reward: -55.0\n",
            "Flight 120 -> action: Change path | state: (0, 0, 4, 1, 4) | reward: -60.0\n",
            "Flight 121 -> action: Change path | state: (2, 2, 2, 1, 4) | reward: -60.0\n",
            "Flight 122 -> action: Swap aircraft | state: (0, 0, 1, 1, 2) | reward: -65.0\n",
            "Flight 123 -> action: Change path | state: (4, 4, 1, 3, 0) | reward: -65.0\n",
            "Flight 124 -> action: Change path | state: (4, 2, 0, 3, 2) | reward: -50.0\n",
            "Flight 125 -> action: Change path | state: (1, 3, 2, 0, 2) | reward: -65.0\n",
            "Flight 126 -> action: Change path | state: (3, 3, 2, 3, 4) | reward: -65.0\n",
            "Flight 127 -> action: Change path | state: (0, 0, 2, 1, 4) | reward: -50.0\n",
            "Flight 128 -> action: Change path | state: (0, 2, 3, 1, 2) | reward: -65.0\n",
            "Flight 129 -> action: Swap aircraft | state: (3, 0, 4, 0, 1) | reward: -80.0\n",
            "Flight 130 -> action: Change path | state: (3, 4, 4, 1, 1) | reward: -80.0\n",
            "Flight 131 -> action: Change path | state: (2, 2, 0, 1, 3) | reward: -50.0\n",
            "Flight 132 -> action: Change path | state: (2, 4, 4, 2, 4) | reward: -80.0\n",
            "Flight 133 -> action: Change path | state: (4, 3, 2, 0, 3) | reward: -65.0\n",
            "Flight 134 -> action: Change path | state: (3, 1, 3, 1, 3) | reward: -60.0\n",
            "Flight 135 -> action: Change path | state: (3, 1, 1, 2, 3) | reward: -50.0\n",
            "Flight 136 -> action: Change path | state: (1, 1, 1, 0, 0) | reward: -50.0\n",
            "Flight 137 -> action: Change path | state: (2, 0, 4, 3, 4) | reward: -60.0\n",
            "Flight 138 -> action: Change path | state: (0, 3, 4, 1, 4) | reward: -75.0\n",
            "Flight 139 -> action: Change path | state: (1, 1, 2, 1, 4) | reward: -55.0\n",
            "Flight 140 -> action: Change path | state: (1, 2, 2, 3, 0) | reward: -60.0\n",
            "Flight 141 -> action: Change path | state: (3, 1, 2, 2, 0) | reward: -55.0\n",
            "Flight 142 -> action: Change path | state: (3, 0, 3, 3, 4) | reward: -55.0\n",
            "Flight 143 -> action: Change path | state: (0, 1, 4, 0, 0) | reward: -65.0\n",
            "Flight 144 -> action: Change path | state: (1, 4, 0, 3, 0) | reward: -60.0\n",
            "Flight 145 -> action: Change path | state: (3, 1, 3, 3, 4) | reward: -60.0\n",
            "Flight 146 -> action: Change path | state: (3, 3, 3, 2, 4) | reward: -70.0\n",
            "Flight 147 -> action: Change path | state: (0, 0, 3, 0, 2) | reward: -55.0\n",
            "Flight 148 -> action: Change path | state: (3, 0, 4, 1, 3) | reward: -60.0\n",
            "Flight 149 -> action: Change path | state: (3, 4, 3, 1, 3) | reward: -75.0\n",
            "Flight 150 -> action: Change path | state: (1, 3, 0, 1, 4) | reward: -55.0\n",
            "Flight 151 -> action: Change path | state: (4, 2, 3, 1, 1) | reward: -65.0\n",
            "Flight 152 -> action: Change path | state: (1, 0, 2, 0, 4) | reward: -50.0\n",
            "Flight 153 -> action: Change path | state: (4, 2, 2, 3, 1) | reward: -60.0\n",
            "Flight 154 -> action: Change path | state: (3, 1, 4, 0, 1) | reward: -65.0\n",
            "Flight 155 -> action: Change path | state: (2, 0, 4, 2, 0) | reward: -60.0\n",
            "Flight 156 -> action: Change path | state: (3, 4, 4, 0, 0) | reward: -80.0\n",
            "Total reward: -9470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN:\n",
        "    def __init__(self, state_bins, action_size, hidden_size=64, lr=0.01, gamma=0.95):\n",
        "        self.state_bins = state_bins\n",
        "        self.action_size = action_size\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        input_size = len(state_bins)\n",
        "        self.w1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros(hidden_size)\n",
        "        self.w2 = np.random.randn(hidden_size, action_size) * 0.01\n",
        "        self.b2 = np.zeros(action_size)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        z1 = x @ self.w1 + self.b1\n",
        "        h1 = np.maximum(z1, 0)\n",
        "        q = h1 @ self.w2 + self.b2\n",
        "        return q, h1, z1\n",
        "\n",
        "    def predict(self, x):\n",
        "        q, _, _ = self._forward(x)\n",
        "        return q\n",
        "\n",
        "    def update(self, batch):\n",
        "        for state, action, reward, next_state, done in batch:\n",
        "            q, h1, z1 = self._forward(state)\n",
        "            target = reward\n",
        "            if not done:\n",
        "                next_q = self.predict(next_state)\n",
        "                target += self.gamma * np.max(next_q)\n",
        "            dq = np.zeros_like(q)\n",
        "            dq[action] = q[action] - target\n",
        "            grad_w2 = np.outer(h1, dq)\n",
        "            grad_b2 = dq\n",
        "            grad_h1 = dq @ self.w2.T\n",
        "            grad_z1 = grad_h1 * (z1 > 0)\n",
        "            grad_w1 = np.outer(state, grad_z1)\n",
        "            grad_b1 = grad_z1\n",
        "            self.w2 -= self.lr * grad_w2\n",
        "            self.b2 -= self.lr * grad_b2\n",
        "            self.w1 -= self.lr * grad_w1\n",
        "            self.b1 -= self.lr * grad_b1\n",
        "\n",
        "\n",
        "def normalize(state, bins):\n",
        "    return np.array(state) / (bins - 1)\n",
        "\n",
        "\n",
        "def train_dqn(env, episodes=200, batch_size=32, buffer_limit=10000, epsilon=1.0, epsilon_decay=0.995):\n",
        "    dqn = DQN(env.obs_bins, env.action_space.n)\n",
        "    replay = []\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        state = normalize(state, env.obs_bins)\n",
        "        done = False\n",
        "        while not done:\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                q = dqn.predict(state)\n",
        "                action = int(np.argmax(q))\n",
        "            next_state, reward, terminated, _, _ = env.step(action)\n",
        "            next_state_n = normalize(next_state, env.obs_bins)\n",
        "            replay.append((state, action, reward, next_state_n, terminated))\n",
        "            if len(replay) > buffer_limit:\n",
        "                replay.pop(0)\n",
        "            if len(replay) >= batch_size:\n",
        "                batch_idx = np.random.choice(len(replay), batch_size, replace=False)\n",
        "                batch = [replay[i] for i in batch_idx]\n",
        "                dqn.update(batch)\n",
        "            state = next_state_n\n",
        "            done = terminated\n",
        "        if epsilon > 0.1:\n",
        "            epsilon *= epsilon_decay\n",
        "    return dqn\n",
        "\n",
        "\n",
        "def evaluate_dqn(env, dqn):\n",
        "    action_names = [\n",
        "        \"Change path\",\n",
        "        \"Swap aircraft\",\n",
        "        \"Cancel flight\",\n",
        "        \"Adjust altitude\",\n",
        "        \"Divert\",\n",
        "        \"Wait\",\n",
        "    ]\n",
        "    state, _ = env.reset()\n",
        "    state_n = normalize(state, env.obs_bins)\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action = int(np.argmax(dqn.predict(state_n)))\n",
        "        next_state, reward, terminated, _, _ = env.step(action)\n",
        "        print(\n",
        "            f\"Flight {state[0]} -> action: {action_names[action]} | state: {state[1:]} | reward: {reward:.1f}\"\n",
        "        )\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        state_n = normalize(state, env.obs_bins)\n",
        "        done = terminated\n",
        "    print(\"Total reward:\", total_reward)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = FlightReroutingEnv(\"flight_schedule_new.xlsx\")\n",
        "    agent = train_dqn(env, episodes=100)\n",
        "    print(\"\\nPolicy rollout:\\n\")\n",
        "    evaluate_dqn(env, agent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgCSc7g2M-KO",
        "outputId": "bc57d495-f0a8-4ae4-b942-37817fe761f5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Policy rollout:\n",
            "\n",
            "Flight 0 -> action: Adjust altitude | state: (4, 1, 2, 2, 3) | reward: -55.0\n",
            "Flight 1 -> action: Adjust altitude | state: (3, 4, 2, 1, 2) | reward: -70.0\n",
            "Flight 2 -> action: Adjust altitude | state: (3, 4, 1, 3, 4) | reward: -65.0\n",
            "Flight 3 -> action: Adjust altitude | state: (2, 1, 2, 1, 2) | reward: -55.0\n",
            "Flight 4 -> action: Adjust altitude | state: (1, 4, 1, 3, 0) | reward: -65.0\n",
            "Flight 5 -> action: Adjust altitude | state: (1, 1, 2, 2, 2) | reward: -55.0\n",
            "Flight 6 -> action: Adjust altitude | state: (2, 1, 0, 2, 0) | reward: -45.0\n",
            "Flight 7 -> action: Adjust altitude | state: (2, 1, 4, 2, 1) | reward: -65.0\n",
            "Flight 8 -> action: Adjust altitude | state: (2, 4, 1, 3, 2) | reward: -65.0\n",
            "Flight 9 -> action: Adjust altitude | state: (4, 4, 4, 3, 1) | reward: -80.0\n",
            "Flight 10 -> action: Adjust altitude | state: (4, 2, 0, 3, 3) | reward: -50.0\n",
            "Flight 11 -> action: Adjust altitude | state: (0, 0, 4, 1, 3) | reward: -60.0\n",
            "Flight 12 -> action: Adjust altitude | state: (4, 2, 1, 2, 2) | reward: -55.0\n",
            "Flight 13 -> action: Adjust altitude | state: (0, 0, 0, 2, 3) | reward: -40.0\n",
            "Flight 14 -> action: Adjust altitude | state: (1, 2, 2, 3, 2) | reward: -60.0\n",
            "Flight 15 -> action: Adjust altitude | state: (2, 0, 0, 1, 4) | reward: -40.0\n",
            "Flight 16 -> action: Adjust altitude | state: (3, 3, 0, 3, 2) | reward: -55.0\n",
            "Flight 17 -> action: Adjust altitude | state: (1, 3, 4, 1, 1) | reward: -75.0\n",
            "Flight 18 -> action: Adjust altitude | state: (0, 2, 2, 3, 1) | reward: -60.0\n",
            "Flight 19 -> action: Adjust altitude | state: (2, 4, 1, 0, 3) | reward: -65.0\n",
            "Flight 20 -> action: Adjust altitude | state: (3, 1, 2, 2, 0) | reward: -55.0\n",
            "Flight 21 -> action: Adjust altitude | state: (2, 2, 4, 0, 2) | reward: -70.0\n",
            "Flight 22 -> action: Adjust altitude | state: (4, 0, 0, 2, 4) | reward: -40.0\n",
            "Flight 23 -> action: Adjust altitude | state: (1, 1, 0, 3, 0) | reward: -45.0\n",
            "Flight 24 -> action: Adjust altitude | state: (1, 2, 3, 0, 4) | reward: -65.0\n",
            "Flight 25 -> action: Adjust altitude | state: (1, 3, 1, 0, 3) | reward: -60.0\n",
            "Flight 26 -> action: Adjust altitude | state: (0, 1, 1, 0, 0) | reward: -50.0\n",
            "Flight 27 -> action: Adjust altitude | state: (1, 4, 3, 2, 3) | reward: -75.0\n",
            "Flight 28 -> action: Adjust altitude | state: (4, 1, 0, 2, 2) | reward: -45.0\n",
            "Flight 29 -> action: Adjust altitude | state: (3, 1, 3, 2, 1) | reward: -60.0\n",
            "Flight 30 -> action: Adjust altitude | state: (4, 2, 0, 1, 3) | reward: -50.0\n",
            "Flight 31 -> action: Adjust altitude | state: (4, 0, 1, 3, 3) | reward: -45.0\n",
            "Flight 32 -> action: Adjust altitude | state: (2, 4, 1, 0, 4) | reward: -65.0\n",
            "Flight 33 -> action: Adjust altitude | state: (2, 2, 1, 3, 0) | reward: -55.0\n",
            "Flight 34 -> action: Adjust altitude | state: (1, 1, 2, 0, 0) | reward: -55.0\n",
            "Flight 35 -> action: Adjust altitude | state: (1, 1, 2, 1, 3) | reward: -55.0\n",
            "Flight 36 -> action: Adjust altitude | state: (4, 1, 3, 1, 4) | reward: -60.0\n",
            "Flight 37 -> action: Adjust altitude | state: (3, 4, 3, 0, 3) | reward: -75.0\n",
            "Flight 38 -> action: Adjust altitude | state: (2, 0, 3, 0, 1) | reward: -55.0\n",
            "Flight 39 -> action: Adjust altitude | state: (4, 2, 0, 2, 3) | reward: -50.0\n",
            "Flight 40 -> action: Adjust altitude | state: (1, 3, 0, 0, 4) | reward: -55.0\n",
            "Flight 41 -> action: Adjust altitude | state: (3, 2, 1, 0, 0) | reward: -55.0\n",
            "Flight 42 -> action: Adjust altitude | state: (3, 1, 0, 0, 0) | reward: -45.0\n",
            "Flight 43 -> action: Adjust altitude | state: (1, 3, 0, 1, 3) | reward: -55.0\n",
            "Flight 44 -> action: Adjust altitude | state: (4, 4, 3, 1, 4) | reward: -75.0\n",
            "Flight 45 -> action: Adjust altitude | state: (3, 0, 4, 3, 3) | reward: -60.0\n",
            "Flight 46 -> action: Adjust altitude | state: (4, 4, 3, 2, 1) | reward: -75.0\n",
            "Flight 47 -> action: Adjust altitude | state: (0, 2, 4, 1, 0) | reward: -70.0\n",
            "Flight 48 -> action: Adjust altitude | state: (2, 2, 4, 0, 4) | reward: -70.0\n",
            "Flight 49 -> action: Adjust altitude | state: (3, 3, 3, 1, 0) | reward: -70.0\n",
            "Flight 50 -> action: Adjust altitude | state: (3, 1, 3, 1, 1) | reward: -60.0\n",
            "Flight 51 -> action: Adjust altitude | state: (2, 2, 2, 1, 0) | reward: -60.0\n",
            "Flight 52 -> action: Adjust altitude | state: (0, 1, 0, 3, 4) | reward: -45.0\n",
            "Flight 53 -> action: Adjust altitude | state: (1, 2, 3, 0, 0) | reward: -65.0\n",
            "Flight 54 -> action: Adjust altitude | state: (0, 1, 0, 1, 0) | reward: -45.0\n",
            "Flight 55 -> action: Adjust altitude | state: (2, 1, 1, 2, 2) | reward: -50.0\n",
            "Flight 56 -> action: Adjust altitude | state: (1, 0, 2, 1, 0) | reward: -50.0\n",
            "Flight 57 -> action: Adjust altitude | state: (2, 2, 1, 1, 4) | reward: -55.0\n",
            "Flight 58 -> action: Adjust altitude | state: (4, 0, 0, 0, 2) | reward: -40.0\n",
            "Flight 59 -> action: Adjust altitude | state: (2, 4, 1, 2, 2) | reward: -65.0\n",
            "Flight 60 -> action: Adjust altitude | state: (4, 2, 4, 0, 2) | reward: -70.0\n",
            "Flight 61 -> action: Adjust altitude | state: (2, 4, 0, 1, 3) | reward: -60.0\n",
            "Flight 62 -> action: Adjust altitude | state: (1, 0, 3, 1, 4) | reward: -55.0\n",
            "Flight 63 -> action: Adjust altitude | state: (0, 4, 1, 1, 3) | reward: -65.0\n",
            "Flight 64 -> action: Adjust altitude | state: (3, 1, 0, 0, 1) | reward: -45.0\n",
            "Flight 65 -> action: Adjust altitude | state: (4, 2, 0, 2, 0) | reward: -50.0\n",
            "Flight 66 -> action: Adjust altitude | state: (0, 0, 3, 0, 0) | reward: -55.0\n",
            "Flight 67 -> action: Adjust altitude | state: (0, 1, 4, 2, 1) | reward: -65.0\n",
            "Flight 68 -> action: Adjust altitude | state: (4, 2, 3, 1, 0) | reward: -65.0\n",
            "Flight 69 -> action: Adjust altitude | state: (4, 3, 4, 3, 2) | reward: -75.0\n",
            "Flight 70 -> action: Adjust altitude | state: (3, 1, 3, 0, 3) | reward: -60.0\n",
            "Flight 71 -> action: Adjust altitude | state: (1, 4, 4, 3, 3) | reward: -80.0\n",
            "Flight 72 -> action: Adjust altitude | state: (2, 4, 2, 3, 3) | reward: -70.0\n",
            "Flight 73 -> action: Adjust altitude | state: (2, 3, 1, 3, 0) | reward: -60.0\n",
            "Flight 74 -> action: Adjust altitude | state: (3, 4, 3, 2, 4) | reward: -75.0\n",
            "Flight 75 -> action: Adjust altitude | state: (0, 1, 4, 2, 2) | reward: -65.0\n",
            "Flight 76 -> action: Adjust altitude | state: (0, 2, 3, 0, 3) | reward: -65.0\n",
            "Flight 77 -> action: Adjust altitude | state: (0, 4, 3, 0, 0) | reward: -75.0\n",
            "Flight 78 -> action: Adjust altitude | state: (3, 1, 3, 3, 4) | reward: -60.0\n",
            "Flight 79 -> action: Adjust altitude | state: (1, 1, 0, 0, 3) | reward: -45.0\n",
            "Flight 80 -> action: Adjust altitude | state: (1, 2, 0, 3, 4) | reward: -50.0\n",
            "Flight 81 -> action: Adjust altitude | state: (3, 3, 4, 0, 2) | reward: -75.0\n",
            "Flight 82 -> action: Adjust altitude | state: (4, 3, 0, 3, 4) | reward: -55.0\n",
            "Flight 83 -> action: Adjust altitude | state: (2, 1, 3, 3, 4) | reward: -60.0\n",
            "Flight 84 -> action: Adjust altitude | state: (3, 4, 2, 0, 3) | reward: -70.0\n",
            "Flight 85 -> action: Adjust altitude | state: (1, 0, 0, 2, 2) | reward: -40.0\n",
            "Flight 86 -> action: Adjust altitude | state: (0, 3, 3, 1, 0) | reward: -70.0\n",
            "Flight 87 -> action: Adjust altitude | state: (1, 4, 3, 0, 2) | reward: -75.0\n",
            "Flight 88 -> action: Adjust altitude | state: (1, 3, 0, 0, 4) | reward: -55.0\n",
            "Flight 89 -> action: Adjust altitude | state: (0, 2, 1, 3, 2) | reward: -55.0\n",
            "Flight 90 -> action: Adjust altitude | state: (1, 2, 0, 2, 0) | reward: -50.0\n",
            "Flight 91 -> action: Adjust altitude | state: (0, 4, 2, 3, 0) | reward: -70.0\n",
            "Flight 92 -> action: Adjust altitude | state: (2, 1, 0, 2, 4) | reward: -45.0\n",
            "Flight 93 -> action: Adjust altitude | state: (1, 3, 1, 0, 2) | reward: -60.0\n",
            "Flight 94 -> action: Adjust altitude | state: (2, 4, 0, 1, 2) | reward: -60.0\n",
            "Flight 95 -> action: Adjust altitude | state: (4, 3, 4, 1, 4) | reward: -75.0\n",
            "Flight 96 -> action: Adjust altitude | state: (3, 4, 3, 0, 1) | reward: -75.0\n",
            "Flight 97 -> action: Adjust altitude | state: (4, 3, 0, 3, 3) | reward: -55.0\n",
            "Flight 98 -> action: Adjust altitude | state: (2, 4, 2, 1, 3) | reward: -70.0\n",
            "Flight 99 -> action: Adjust altitude | state: (0, 0, 1, 2, 1) | reward: -45.0\n",
            "Flight 100 -> action: Adjust altitude | state: (1, 2, 1, 3, 2) | reward: -55.0\n",
            "Flight 101 -> action: Adjust altitude | state: (1, 4, 4, 0, 0) | reward: -80.0\n",
            "Flight 102 -> action: Adjust altitude | state: (0, 0, 4, 2, 2) | reward: -60.0\n",
            "Flight 103 -> action: Adjust altitude | state: (4, 3, 0, 3, 4) | reward: -55.0\n",
            "Flight 104 -> action: Adjust altitude | state: (1, 0, 1, 0, 1) | reward: -45.0\n",
            "Flight 105 -> action: Adjust altitude | state: (2, 1, 0, 1, 1) | reward: -45.0\n",
            "Flight 106 -> action: Adjust altitude | state: (2, 4, 0, 1, 2) | reward: -60.0\n",
            "Flight 107 -> action: Adjust altitude | state: (3, 3, 0, 1, 4) | reward: -55.0\n",
            "Flight 108 -> action: Adjust altitude | state: (4, 1, 4, 1, 0) | reward: -65.0\n",
            "Flight 109 -> action: Adjust altitude | state: (2, 2, 0, 1, 2) | reward: -50.0\n",
            "Flight 110 -> action: Adjust altitude | state: (2, 2, 1, 0, 3) | reward: -55.0\n",
            "Flight 111 -> action: Adjust altitude | state: (2, 4, 2, 1, 0) | reward: -70.0\n",
            "Flight 112 -> action: Adjust altitude | state: (3, 0, 1, 0, 4) | reward: -45.0\n",
            "Flight 113 -> action: Adjust altitude | state: (1, 4, 1, 1, 3) | reward: -65.0\n",
            "Flight 114 -> action: Adjust altitude | state: (1, 0, 1, 0, 3) | reward: -45.0\n",
            "Flight 115 -> action: Adjust altitude | state: (1, 3, 3, 0, 4) | reward: -70.0\n",
            "Flight 116 -> action: Adjust altitude | state: (0, 2, 0, 0, 4) | reward: -50.0\n",
            "Flight 117 -> action: Adjust altitude | state: (2, 1, 0, 0, 2) | reward: -45.0\n",
            "Flight 118 -> action: Adjust altitude | state: (3, 0, 0, 3, 1) | reward: -40.0\n",
            "Flight 119 -> action: Adjust altitude | state: (2, 3, 1, 0, 4) | reward: -60.0\n",
            "Flight 120 -> action: Adjust altitude | state: (0, 1, 1, 1, 3) | reward: -50.0\n",
            "Flight 121 -> action: Adjust altitude | state: (0, 1, 1, 2, 2) | reward: -50.0\n",
            "Flight 122 -> action: Adjust altitude | state: (1, 3, 2, 2, 4) | reward: -65.0\n",
            "Flight 123 -> action: Adjust altitude | state: (1, 0, 4, 3, 0) | reward: -60.0\n",
            "Flight 124 -> action: Adjust altitude | state: (1, 2, 2, 2, 2) | reward: -60.0\n",
            "Flight 125 -> action: Adjust altitude | state: (4, 0, 3, 3, 3) | reward: -55.0\n",
            "Flight 126 -> action: Adjust altitude | state: (0, 4, 3, 3, 3) | reward: -75.0\n",
            "Flight 127 -> action: Adjust altitude | state: (0, 0, 4, 2, 3) | reward: -60.0\n",
            "Flight 128 -> action: Adjust altitude | state: (1, 1, 0, 1, 4) | reward: -45.0\n",
            "Flight 129 -> action: Adjust altitude | state: (4, 1, 4, 1, 2) | reward: -65.0\n",
            "Flight 130 -> action: Adjust altitude | state: (3, 1, 1, 1, 2) | reward: -50.0\n",
            "Flight 131 -> action: Adjust altitude | state: (0, 3, 4, 3, 0) | reward: -75.0\n",
            "Flight 132 -> action: Adjust altitude | state: (1, 4, 2, 1, 4) | reward: -70.0\n",
            "Flight 133 -> action: Adjust altitude | state: (0, 2, 2, 2, 0) | reward: -60.0\n",
            "Flight 134 -> action: Adjust altitude | state: (2, 4, 1, 2, 3) | reward: -65.0\n",
            "Flight 135 -> action: Adjust altitude | state: (3, 4, 0, 3, 2) | reward: -60.0\n",
            "Flight 136 -> action: Adjust altitude | state: (1, 2, 2, 2, 2) | reward: -60.0\n",
            "Flight 137 -> action: Adjust altitude | state: (1, 0, 1, 2, 2) | reward: -45.0\n",
            "Flight 138 -> action: Adjust altitude | state: (2, 4, 2, 0, 1) | reward: -70.0\n",
            "Flight 139 -> action: Adjust altitude | state: (3, 4, 1, 3, 4) | reward: -65.0\n",
            "Flight 140 -> action: Adjust altitude | state: (2, 4, 2, 1, 1) | reward: -70.0\n",
            "Flight 141 -> action: Adjust altitude | state: (2, 3, 1, 0, 1) | reward: -60.0\n",
            "Flight 142 -> action: Adjust altitude | state: (4, 2, 3, 2, 0) | reward: -65.0\n",
            "Flight 143 -> action: Adjust altitude | state: (1, 0, 3, 0, 4) | reward: -55.0\n",
            "Flight 144 -> action: Adjust altitude | state: (3, 1, 3, 2, 1) | reward: -60.0\n",
            "Flight 145 -> action: Adjust altitude | state: (1, 4, 3, 1, 4) | reward: -75.0\n",
            "Flight 146 -> action: Adjust altitude | state: (0, 4, 2, 1, 2) | reward: -70.0\n",
            "Flight 147 -> action: Adjust altitude | state: (1, 1, 0, 3, 4) | reward: -45.0\n",
            "Flight 148 -> action: Adjust altitude | state: (2, 0, 1, 3, 0) | reward: -45.0\n",
            "Flight 149 -> action: Adjust altitude | state: (1, 1, 2, 0, 4) | reward: -55.0\n",
            "Flight 150 -> action: Adjust altitude | state: (1, 4, 1, 0, 1) | reward: -65.0\n",
            "Flight 151 -> action: Adjust altitude | state: (1, 3, 1, 3, 4) | reward: -60.0\n",
            "Flight 152 -> action: Adjust altitude | state: (0, 4, 2, 2, 3) | reward: -70.0\n",
            "Flight 153 -> action: Adjust altitude | state: (3, 1, 0, 1, 4) | reward: -45.0\n",
            "Flight 154 -> action: Adjust altitude | state: (1, 3, 0, 0, 4) | reward: -55.0\n",
            "Flight 155 -> action: Adjust altitude | state: (2, 1, 2, 1, 0) | reward: -55.0\n",
            "Flight 156 -> action: Adjust altitude | state: (3, 2, 3, 1, 4) | reward: -65.0\n",
            "Total reward: -9280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_schedule(env, agent, epsilon=0.05):\n",
        "    \"\"\"Generate a schedule (action for each flight) using an epsilon-greedy policy.\"\"\"\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    actions = []\n",
        "    total_reward = 0.0\n",
        "    while not done:\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = agent.choose_action(state)\n",
        "        next_state, reward, terminated, _, _ = env.step(action)\n",
        "        actions.append(action)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        done = terminated\n",
        "    return actions, total_reward\n",
        "\n",
        "\n",
        "def column_generation(schedule_path, iterations=20):\n",
        "    \"\"\"Run column generation enhanced by an RL agent.\"\"\"\n",
        "    env = FlightReroutingEnv(schedule_path)\n",
        "    agent = train(env)\n",
        "\n",
        "    columns = []\n",
        "    best_actions = None\n",
        "    best_cost = float(\"inf\")\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        actions, reward = generate_schedule(env, agent)\n",
        "        cost = -reward\n",
        "        columns.append((actions, cost))\n",
        "        if cost < best_cost:\n",
        "            best_cost = cost\n",
        "            best_actions = actions\n",
        "\n",
        "    return columns, best_actions, best_cost\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cols, best_sched, best_cost = column_generation(\"flight_schedule_new.xlsx\")\n",
        "    print(\"Generated columns:\", len(cols))\n",
        "    print(\"Best schedule cost:\", best_cost)\n",
        "    print(\"Best schedule actions:\", best_sched)"
      ],
      "metadata": {
        "id": "HodLqmUOU3xI",
        "outputId": "e99d1839-3f6b-474d-9c86-945c3c75ce64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated columns: 20\n",
            "Best schedule cost: 9760.0\n",
            "Best schedule actions: [0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, np.int64(0), 0, np.int64(3), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, np.int64(0), 0, 1, 0, 4, np.int64(1), 0, 0, 1, 3, 0, 1, 0, 0, 0, 1, 0, np.int64(0), np.int64(5), 0, 0, 0, 1, 1, 0, 0, np.int64(1), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, np.int64(3), 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4, 0, 4, np.int64(3), 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0, 0, 1, 0, np.int64(5), 0, 0, 0, 0, 0, np.int64(3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_schedule(env, dqn, epsilon=0.05):\n",
        "    \"\"\"Generate a schedule using an epsilon-greedy policy with the DQN.\"\"\"\n",
        "    state, _ = env.reset()\n",
        "    state_n = normalize(state, env.obs_bins)\n",
        "    done = False\n",
        "    actions = []\n",
        "    total_reward = 0.0\n",
        "    while not done:\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = int(np.argmax(dqn.predict(state_n)))\n",
        "        next_state, reward, terminated, _, _ = env.step(action)\n",
        "        actions.append(action)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        state_n = normalize(state, env.obs_bins)\n",
        "        done = terminated\n",
        "    return actions, total_reward\n",
        "\n",
        "\n",
        "def column_generation(schedule_path, iterations=20):\n",
        "    \"\"\"Run column generation enhanced with a DQN agent.\"\"\"\n",
        "    env = FlightReroutingEnv(schedule_path)\n",
        "    dqn = train_dqn(env)\n",
        "\n",
        "    columns = []\n",
        "    best_actions = None\n",
        "    best_cost = float(\"inf\")\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        actions, reward = generate_schedule(env, dqn)\n",
        "        cost = -reward\n",
        "        columns.append((actions, cost))\n",
        "        if cost < best_cost:\n",
        "            best_cost = cost\n",
        "            best_actions = actions\n",
        "\n",
        "    return columns, best_actions, best_cost\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cols, best_sched, best_cost = column_generation(\"flight_schedule_new.xlsx\")\n",
        "    print(\"Generated columns:\", len(cols))\n",
        "    print(\"Best schedule cost:\", best_cost)\n",
        "    print(\"Best schedule actions:\", best_sched)"
      ],
      "metadata": {
        "id": "71kWhgXeIBIj",
        "outputId": "74eb260b-bb5a-487b-f2ad-22a2d87d4778",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated columns: 20\n",
            "Best schedule cost: 9320.0\n",
            "Best schedule actions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, np.int64(3), 0, np.int64(1), 0, 0, 0, 0, 0, 0, np.int64(5), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, np.int64(1), 0, 0, 0, 0, 0, np.int64(3), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, np.int64(3), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, np.int64(4), 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DVPS2Or0NKOl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}