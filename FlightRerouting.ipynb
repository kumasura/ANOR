{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlaZCyLt4SzRvvN8iJDiyp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumasura/ANOR/blob/main/FlightRerouting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZF_SacieJ_DA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FlightReroutingEnv(gym.Env):\n",
        "\n",
        "    \"\"\"Environment using schedule data with simple simulated disruptions.\"\"\"\n",
        "    def __init__(self, schedule_path, disruption_prob=0.3):\n",
        "\n",
        "        super().__init__()\n",
        "        self.schedule = pd.read_excel(schedule_path)\n",
        "        self.num_flights = len(self.schedule)\n",
        "        self.disruption_prob = disruption_prob\n",
        "\n",
        "\n",
        "        # Observation consists of: flight index, fuel level, weather, traffic,\n",
        "        # alternate airports, other aircraft proximity\n",
        "        self.obs_bins = np.array([\n",
        "            self.num_flights + 1,  # flight index including terminal state\n",
        "            5,  # fuel level bins\n",
        "            5,  # weather bins\n",
        "            5,  # traffic bins\n",
        "            4,  # number of alternate airports\n",
        "            5,  # other aircraft proximity bins\n",
        "        ])\n",
        "        self.observation_space = spaces.MultiDiscrete(self.obs_bins)\n",
        "\n",
        "        # Actions follow the README: change path, swap aircraft, cancel, adjust\n",
        "        # altitude, divert, wait for conditions to improve\n",
        "        self.action_space = spaces.Discrete(6)\n",
        "\n",
        "    def _random_state(self):\n",
        "        return [\n",
        "            np.random.randint(self.obs_bins[1]),\n",
        "            np.random.randint(self.obs_bins[2]),\n",
        "            np.random.randint(self.obs_bins[3]),\n",
        "            np.random.randint(self.obs_bins[4]),\n",
        "            np.random.randint(self.obs_bins[5]),\n",
        "        ]\n",
        "\n",
        "\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_idx = 0\n",
        "\n",
        "        self.state = self._random_state()\n",
        "        return tuple([self.current_idx] + self.state), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.current_idx >= self.num_flights:\n",
        "            raise RuntimeError(\"Episode is done\")\n",
        "\n",
        "        disruption = np.random.rand() < self.disruption_prob\n",
        "        fuel, weather, traffic, airports, other = self.state\n",
        "\n",
        "        # Simple cost model\n",
        "        fuel_cost = 10 + 5 * weather + 5 * traffic\n",
        "        delay_penalty = 20 if action == 5 else 0\n",
        "        swap_penalty = 50 if action == 1 else 0\n",
        "        cancel_penalty = 200 if action == 2 else 0\n",
        "        reroute_penalty = 30 if action in (0, 3, 4) else 0\n",
        "        if not disruption and action == 5:\n",
        "            delay_penalty += 20  # unnecessary waiting\n",
        "\n",
        "        reward = -(fuel_cost + delay_penalty + swap_penalty + cancel_penalty + reroute_penalty)\n",
        "\n",
        "        # Advance to next flight and generate new state\n",
        "        self.current_idx += 1\n",
        "        terminated = self.current_idx >= self.num_flights\n",
        "        self.state = self._random_state() if not terminated else [0] * 5\n",
        "        obs = tuple([self.num_flights] + self.state) if terminated else tuple([self.current_idx] + self.state)\n",
        "        return obs, reward, terminated, False, {}"
      ],
      "metadata": {
        "id": "c6aVg2RrMihZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, nvec, action_size, alpha=0.1, gamma=0.95, epsilon=0.1):\n",
        "        state_space = int(np.prod(nvec))\n",
        "        self.Q = np.zeros((state_space, action_size))\n",
        "        self.nvec = nvec\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.action_size = action_size\n",
        "\n",
        "\n",
        "    def _state_index(self, state):\n",
        "        return np.ravel_multi_index(state, self.nvec)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        idx = self._state_index(state)\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "        return int(np.argmax(self.Q[idx]))\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        idx = self._state_index(state)\n",
        "        next_idx = self._state_index(next_state)\n",
        "        best_next = np.max(self.Q[next_idx])\n",
        "        td_target = reward + self.gamma * best_next\n",
        "        td_error = td_target - self.Q[idx, action]\n",
        "        self.Q[idx, action] += self.alpha * td_error\n"
      ],
      "metadata": {
        "id": "FaO_JzKBMz7m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(env, agent):\n",
        "    \"\"\"Run one episode with the learned policy and print step details.\"\"\"\n",
        "    action_names = [\n",
        "        \"Change path\",\n",
        "        \"Swap aircraft\",\n",
        "        \"Cancel flight\",\n",
        "        \"Adjust altitude\",\n",
        "        \"Divert\",\n",
        "        \"Wait\",\n",
        "    ]\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action = int(np.argmax(agent.Q[agent._state_index(state)]))\n",
        "        next_state, reward, terminated, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        print(\n",
        "            f\"Flight {state[0]} -> action: {action_names[action]} | \"\n",
        "            f\"state: {state[1:]} | reward: {reward:.1f}\"\n",
        "        )\n",
        "        state = next_state\n",
        "        done = terminated\n",
        "    print(\"Total reward:\", total_reward)\n"
      ],
      "metadata": {
        "id": "ifg1KjJoO1PQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env, episodes=200):\n",
        "    agent = QLearningAgent(env.obs_bins, env.action_space.n)\n",
        "    for _ in range(episodes):\n",
        "\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, _, _ = env.step(action)\n",
        "            agent.update(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "            done = terminated\n",
        "    return agent"
      ],
      "metadata": {
        "id": "Cj6Tiq6oM3OG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    env = FlightReroutingEnv('flight_schedule_new.xlsx')\n",
        "    agent = train(env)\n",
        "    print(\"Q-table shape:\", agent.Q.shape)\n",
        "    print(agent.Q[:5])\n",
        "    print(\"\\nPolicy rollout:\")\n",
        "    evaluate(env, agent)\n"
      ],
      "metadata": {
        "id": "ZZqzflnFM5r2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtTVyY5bM72m",
        "outputId": "16f3c85f-997f-4ba0-c630-58c0f1ea2217"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-table shape: (25000, 6)\n",
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "Policy rollout:\n",
            "Flight 0 -> action: Cancel flight | state: (1, 3, 3, 1, 3) | reward: -240.0\n",
            "Flight 1 -> action: Change path | state: (3, 1, 4, 3, 0) | reward: -65.0\n",
            "Flight 2 -> action: Change path | state: (4, 0, 1, 1, 1) | reward: -45.0\n",
            "Flight 3 -> action: Change path | state: (1, 2, 3, 2, 3) | reward: -65.0\n",
            "Flight 4 -> action: Change path | state: (3, 3, 3, 1, 4) | reward: -70.0\n",
            "Flight 5 -> action: Change path | state: (0, 3, 4, 0, 3) | reward: -75.0\n",
            "Flight 6 -> action: Change path | state: (4, 3, 4, 1, 4) | reward: -75.0\n",
            "Flight 7 -> action: Change path | state: (4, 0, 4, 1, 1) | reward: -60.0\n",
            "Flight 8 -> action: Change path | state: (2, 2, 3, 3, 4) | reward: -65.0\n",
            "Total reward: -760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN:\n",
        "    def __init__(self, state_bins, action_size, hidden_size=64, lr=0.01, gamma=0.95):\n",
        "        self.state_bins = state_bins\n",
        "        self.action_size = action_size\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        input_size = len(state_bins)\n",
        "        self.w1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros(hidden_size)\n",
        "        self.w2 = np.random.randn(hidden_size, action_size) * 0.01\n",
        "        self.b2 = np.zeros(action_size)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        z1 = x @ self.w1 + self.b1\n",
        "        h1 = np.maximum(z1, 0)\n",
        "        q = h1 @ self.w2 + self.b2\n",
        "        return q, h1, z1\n",
        "\n",
        "    def predict(self, x):\n",
        "        q, _, _ = self._forward(x)\n",
        "        return q\n",
        "\n",
        "    def update(self, batch):\n",
        "        for state, action, reward, next_state, done in batch:\n",
        "            q, h1, z1 = self._forward(state)\n",
        "            target = reward\n",
        "            if not done:\n",
        "                next_q = self.predict(next_state)\n",
        "                target += self.gamma * np.max(next_q)\n",
        "            dq = np.zeros_like(q)\n",
        "            dq[action] = q[action] - target\n",
        "            grad_w2 = np.outer(h1, dq)\n",
        "            grad_b2 = dq\n",
        "            grad_h1 = dq @ self.w2.T\n",
        "            grad_z1 = grad_h1 * (z1 > 0)\n",
        "            grad_w1 = np.outer(state, grad_z1)\n",
        "            grad_b1 = grad_z1\n",
        "            self.w2 -= self.lr * grad_w2\n",
        "            self.b2 -= self.lr * grad_b2\n",
        "            self.w1 -= self.lr * grad_w1\n",
        "            self.b1 -= self.lr * grad_b1\n",
        "\n",
        "\n",
        "def normalize(state, bins):\n",
        "    return np.array(state) / (bins - 1)\n",
        "\n",
        "\n",
        "def train_dqn(env, episodes=200, batch_size=32, buffer_limit=10000, epsilon=1.0, epsilon_decay=0.995):\n",
        "    dqn = DQN(env.obs_bins, env.action_space.n)\n",
        "    replay = []\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        state = normalize(state, env.obs_bins)\n",
        "        done = False\n",
        "        while not done:\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                q = dqn.predict(state)\n",
        "                action = int(np.argmax(q))\n",
        "            next_state, reward, terminated, _, _ = env.step(action)\n",
        "            next_state_n = normalize(next_state, env.obs_bins)\n",
        "            replay.append((state, action, reward, next_state_n, terminated))\n",
        "            if len(replay) > buffer_limit:\n",
        "                replay.pop(0)\n",
        "            if len(replay) >= batch_size:\n",
        "                batch_idx = np.random.choice(len(replay), batch_size, replace=False)\n",
        "                batch = [replay[i] for i in batch_idx]\n",
        "                dqn.update(batch)\n",
        "            state = next_state_n\n",
        "            done = terminated\n",
        "        if epsilon > 0.1:\n",
        "            epsilon *= epsilon_decay\n",
        "    return dqn\n",
        "\n",
        "\n",
        "def evaluate_dqn(env, dqn):\n",
        "    action_names = [\n",
        "        \"Change path\",\n",
        "        \"Swap aircraft\",\n",
        "        \"Cancel flight\",\n",
        "        \"Adjust altitude\",\n",
        "        \"Divert\",\n",
        "        \"Wait\",\n",
        "    ]\n",
        "    state, _ = env.reset()\n",
        "    state_n = normalize(state, env.obs_bins)\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action = int(np.argmax(dqn.predict(state_n)))\n",
        "        next_state, reward, terminated, _, _ = env.step(action)\n",
        "        print(\n",
        "            f\"Flight {state[0]} -> action: {action_names[action]} | state: {state[1:]} | reward: {reward:.1f}\"\n",
        "        )\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        state_n = normalize(state, env.obs_bins)\n",
        "        done = terminated\n",
        "    print(\"Total reward:\", total_reward)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = FlightReroutingEnv(\"flight_schedule_new.xlsx\")\n",
        "    agent = train_dqn(env, episodes=100)\n",
        "    print(\"\\nPolicy rollout:\\n\")\n",
        "    evaluate_dqn(env, agent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgCSc7g2M-KO",
        "outputId": "4ef19d8f-a4ab-4bb6-f57f-786951bb8c68"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Policy rollout:\n",
            "\n",
            "Flight 0 -> action: Divert | state: (1, 2, 2, 1, 1) | reward: -60.0\n",
            "Flight 1 -> action: Divert | state: (2, 2, 2, 0, 3) | reward: -60.0\n",
            "Flight 2 -> action: Divert | state: (1, 3, 0, 3, 4) | reward: -55.0\n",
            "Flight 3 -> action: Divert | state: (1, 3, 4, 3, 0) | reward: -75.0\n",
            "Flight 4 -> action: Divert | state: (0, 3, 4, 3, 1) | reward: -75.0\n",
            "Flight 5 -> action: Divert | state: (0, 3, 2, 1, 3) | reward: -65.0\n",
            "Flight 6 -> action: Divert | state: (1, 3, 2, 0, 1) | reward: -65.0\n",
            "Flight 7 -> action: Divert | state: (2, 0, 2, 1, 2) | reward: -50.0\n",
            "Flight 8 -> action: Divert | state: (4, 4, 4, 2, 4) | reward: -80.0\n",
            "Total reward: -585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HodLqmUOU3xI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}